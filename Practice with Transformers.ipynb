{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7a3190a",
   "metadata": {},
   "source": [
    "## Using different decoding methods for language generation with transformers\n",
    "Adapter from https://huggingface.co/blog/how-to-generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707a9d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418c541692b4769a89c5f82d7e23dce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293c44bbb34a441e8a3a28ad4d89f270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a550b1c4d0c940d294081efc4c22bbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45f47279ec143b88dc608ad3c26c256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d15980296f943508336a16b8111247d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 20:37:35.438423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:35.506454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:35.508504: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:35.513024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:35.514489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:35.515614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:36.326950: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:36.327417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:36.327813: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1050] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-09 20:37:36.328185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8103 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2021-11-09 20:37:36.401624: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482a8e1",
   "metadata": {},
   "source": [
    "## Greedy Search\n",
    "#### Select the word with the highest probability as its next word\n",
    "\n",
    "This clearly ends up overfitting and gets locked into certain patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478ab581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters the room.\n",
      "\n",
      "\"I'm sorry, but I'm not going to be able to do this anymore.\"\n",
      "\n",
      "\"I'm sorry, but I'm not going to be able to do this anymore.\"\n",
      "\n",
      "\"I'm\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Mel enters', return_tensors='tf')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d3c33",
   "metadata": {},
   "source": [
    "## Beam Search\n",
    "#### Instead of a single word, chooses most likely combination of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae5ef84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you.\"\n",
      "\n",
      "\"I didn't mean to hurt you.\"\n",
      "\n",
      "\"I didn't mean to hurt you.\"\n",
      "\n",
      "\"I\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd3b659",
   "metadata": {},
   "source": [
    "### N-Gram penalties - create penalties for repeating sequences of n words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692f87b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you.\"\n",
      "\n",
      "\"It's okay,\" he says. \"It's okay.\"\n",
      "\n",
      "She looks at him. \"I'm sorry\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 4\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=4, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ef4950",
   "metadata": {},
   "source": [
    "## Comparing possible sequences\n",
    "#### We can set num_return_sequences to return the top n beams which are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a55465a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you. I just wanted you to know that I love you, and I'm going to do everything in my power to make sure that you\n",
      "\n",
      "1: Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you. I just wanted you to know that I love you, and I'm going to do everything in my power to help you.\"\n",
      "\n",
      "\n",
      "2: Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you. I just wanted you to know that I love you, and I'm going to do everything in my power to make sure you're\n",
      "\n",
      "3: Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you. I just wanted you to know that I love you, and I'm going to do everything in my power to make sure you don\n",
      "\n",
      "4: Mel enters the room.\n",
      "\n",
      "\"I'm sorry,\" she says. \"I didn't mean to hurt you. I just wanted you to know that I love you, and I'm going to do everything in my power to make sure you never\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\\n\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13404c63",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "#### Instead of generating maximum likelihood beams, we can sample words in a weighted fashion. This will give us more randomized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d257edcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters at the helm of human agency, including righteous discovery, consequences before edicts | Roberto Fileno | TODAY Ort Mener weddings 16 children; over 150 pagans run for leadership | WINSmithW\n",
      "\n",
      "Read ORT's latest wrap-\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9132129",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "#### We can change of the temperature of the algorithm to increase or decrease randomness. A lower temperature \"cools down\", aka makes less insane, the words by making chosen words have a higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811caabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters the room and increases the amount of time he can wait for her to attack.\n",
      "\n",
      "After leaving the terminal, she runs a walkthrough to the left, and an open doorsteps outside.\n",
      "\n",
      "The crew of the ship leaves,\n"
     ]
    }
   ],
   "source": [
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70469eb8",
   "metadata": {},
   "source": [
    "## Top-K Sampling\n",
    "#### the K most likely words are filtered, and the probability mass is reditributed among only those K next words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1467534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters The Cavern of the Sunlight. This creature can't be blocked as long as attacking creatures aren't blocked by this creature.\n",
      "\n",
      "Spell Queue When this card is cast, if any of your opponents cast multiple cards from their hand at\n"
     ]
    }
   ],
   "source": [
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d023bf",
   "metadata": {},
   "source": [
    "### Top-p (nucleus) sampling\n",
    "#### Instead of choosing K as the number of words, we choose the amount of words necessary to get us to a cumulative probability exceeding p. So if p=0.95, we choose a bunch of words whose probabilities, when added up, are over 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d99db116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Mel enters his cubicle after\n",
      "\n",
      "taking the offer. He is never given this type of. He breaks into\n",
      "\n",
      "Havenna's 'Sweet At Play'.\n",
      "\n",
      "\n",
      "Octave: Hi, Danny\n",
      "\n",
      "You're new there, aren\n"
     ]
    }
   ],
   "source": [
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f498e0f3",
   "metadata": {},
   "source": [
    "#### We can again set num_return_sequences to more than 1 so we can compare our input parameters better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9dbe8469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Mel enters the court where he has been detained. He is seen holding a police bag before he gets into his van. In the end, the van explodes in the crowd of people standing around.\n",
      "\n",
      "This is the scene at the time when S\n",
      "1: Mel enters the court to receive a sentence of imprisonment of up to three years.\n",
      "\n",
      "\"The sentence shall be suspended until and unless the defendant fails to appear or shows a reasonable excuse for failure or misconduct in court or of the evidence against him and\n",
      "2: Mel enters and uses a power that makes the character's movement a lot faster and faster.\n",
      "\n",
      "Contents show]\n",
      "\n",
      "Effects Edit\n",
      "\n",
      "A Power Spell can be used to temporarily or temporarily cancel the effects of a Power Spell. The affected character\n"
     ]
    }
   ],
   "source": [
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72111b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
